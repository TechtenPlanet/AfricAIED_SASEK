{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBsImOKYbMFe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C-YM1rzbF6a",
        "outputId": "8639aef1-edd6-40bc-dd2c-eaf5148e3b1d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BG0fM3XCa451",
        "outputId": "1cce67fe-2b91-4ce8-f149-bdcb29539e65"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install TTS\n",
        "!sudo apt-get install espeak-ng\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "\n",
        "# STT\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "\n",
        "# API-related dependencies\n",
        "!pip install fastapi uvicorn pydantic pyngrok nest_asyncio\n",
        "!pip install python-multipart\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoOhnH_hbauY"
      },
      "outputs": [],
      "source": [
        "# TTS-Related Imports\n",
        "import IPython\n",
        "import tempfile\n",
        "import subprocess\n",
        "import time\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.audio.numpy_transforms import save_wav\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# STT-Related Imports\n",
        "import io\n",
        "import wave\n",
        "import numpy as np\n",
        "import whisper\n",
        "import jiwer\n",
        "import time\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import joblib\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# API-Related Imports\n",
        "from fastapi import FastAPI,Response\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.responses import StreamingResponse,FileResponse\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import shutil\n",
        "from pydantic import BaseModel\n",
        "from IPython.display import Audio\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfOuc8Pxg8Y_"
      },
      "outputs": [],
      "source": [
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "app.add_middleware(GZipMiddleware, minimum_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlS247Nggdnb"
      },
      "source": [
        "## TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYUzxcTsbg17"
      },
      "outputs": [],
      "source": [
        "# Load TTS model\n",
        "live_config=VitsConfig()\n",
        "live_config.load_json(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/traineroutput/vits_vctk-May-24-2023_11+05PM-23a7a9a3/config.json\")\n",
        "live_vits = Vits.init_from_config(live_config)\n",
        "live_vits.load_onnx(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/elsie.onnx\")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NuZNL3ShiVG"
      },
      "outputs": [],
      "source": [
        "def live_audio(text:str):\n",
        "  text_inputs = np.asarray(\n",
        "      live_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "      dtype=np.int64,\n",
        "  )[None, :]\n",
        "  audio = live_vits.inference_onnx(text_inputs,speaker_id=0)\n",
        "  with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "    out_path = temp_file.name\n",
        "  save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "  return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWQ0Ci43goyB"
      },
      "outputs": [],
      "source": [
        "class LiveText(BaseModel):\n",
        "  text: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhC41XNvhZxr"
      },
      "outputs": [],
      "source": [
        "@app.get('/synthesize-speech')\n",
        "def onnx_audio(payload:LiveText):\n",
        "  out_path=live_audio(payload.text)\n",
        "  return FileResponse(out_path, media_type=\"audio/wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYnYzhfaghmB"
      },
      "source": [
        "## STT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YiZLETpqBP_",
        "outputId": "3b89c522-0271-4189-fa3f-da1ec549f456"
      },
      "outputs": [],
      "source": [
        "# Load STT Model\n",
        "# Load whisper model\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\"\n",
        "\n",
        "model = whisper.load_model(\"medium.en\", device = DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBgmmhkXqHOl"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio(path_to_audio):\n",
        "  \"\"\"Loads whisper model to transcribe audio\"\"\"\n",
        "\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(path_to_audio)\n",
        "\n",
        "  # Transcribe audio\n",
        "  result = model.transcribe(audio)\n",
        "\n",
        "  # Print transcript\n",
        "  return result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMFjRgyQrjrB"
      },
      "outputs": [],
      "source": [
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "@app.get(\"/get-transcript\")\n",
        "async def get_transcript(audio: AudioBytes):\n",
        "  try:\n",
        "    decoded_data = base64.b64decode(audio.data)\n",
        "\n",
        "    # Write bytes data to a .wav file\n",
        "    with io.BytesIO(decoded_data) as audio_file:\n",
        "        with wave.open(audio_file, \"wb\") as wav:\n",
        "          wav.setnchannels(1)\n",
        "          wav.setsampwidth(2)\n",
        "          wav.setframerate(16000)\n",
        "\n",
        "          # Write .wav files\n",
        "          wav.writeframes(decoded_data)\n",
        "\n",
        "    # Save the audio file with the custom name\n",
        "    audio_filename = audio.filename\n",
        "    with open(audio_filename, \"wb\") as file:\n",
        "        file.write(decoded_data)\n",
        "\n",
        "    transcript = transcribe_audio(audio_filename)\n",
        "    os.remove(audio_filename)\n",
        "    return {\"transcript\": transcript}\n",
        "  except Exception as e:\n",
        "    return {\"error\":str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LM fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mount Google drive "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('json', data_files='/content/drive/MyDrive/newdata1.jsonl', split='train')\n",
        "eval_dataset = load_dataset('json', data_files='/content/drive/MyDrive/newdata1.jsonl', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define dataset formatting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authenticate with hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"TheBloke/Mistral-7B-Claude-Chat-GGUF\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    return tokenizer(formatting_func(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 512 # This was an appropriate max length for my dataset\n",
        "\n",
        "def generate_and_tokenize_prompt2(prompt):\n",
        "    result = tokenizer(\n",
        "        formatting_func(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tokenized_train_dataset[1]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Give me an NSMQ question\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init an eval tokenizer that doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input,max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "project = \"journal-finetune\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=2,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training          # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"TheBloke/Mistral-7B-Claude-Chat-GGUF\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"TheBloke/Mistral-7B-Claude-Chat-GGUF/checkpoint-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Message(BaseModel):\n",
        "    input: str\n",
        "    output: Optional[str] = None\n",
        "\n",
        "class InputModel(BaseModel):\n",
        "    messages: List<Message]\n",
        "\n",
        "class OutputModel(BaseModel):\n",
        "    ai_response: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_promt = \"\"\"\n",
        "You are a quiz mistress for the NSMQ quiz competition: \n",
        "\n",
        "This is how the competition occurs,\n",
        "1. There are three teams in the competition\n",
        "2. There are five rounds.\n",
        "3 The first round consists of 24 questions with each school getting two sets of questions from each subject: Math, Chemistry, Physics and Biology.\n",
        "Questions are asked for the different subjects one after the other with one question posed to each school from a set of questions from the same subject. \n",
        "For questions that don't require calculations, the time required is 10 seconds and for questions that require calculations the time is 30seconds.\n",
        "4. The second round consists of 8 questions with each school at liberty of answering immediately they have an answer.\n",
        "There are two questions each for each subject(Math, Chemistry, Physics and Biology) which any team can answer immediately the question is posed.\n",
        "For every wrong answer given, there is a deduction of one point from the accummulated scores.  Th efirst team to answer a question gets the full 3 points.\n",
        "The second team to make a correct attempt after the first team makes an attempt and gets it wrong, gets 2 points and if they make an unsuccessful attempt,\n",
        "the last team after making a successful attempt gets one point.  \n",
        "5. The third round also known as the problem of the day is such that a single question is posed to all three schools and each school has a maximum time of\n",
        "4 minutes to present a solution to the problem. Marks are awarded based on the criteria and presentation of the solution apart from the final answer itself.\n",
        "The total points to be accummulated is 10.\n",
        "6. The fourth round round is also known as the True or False stage. Here, two questions each from each subject(Math, chemistry, Physicics and Biology)\n",
        "are posed to each team. The questions are statements where each team is supposed to state whether the questions are True or False. \n",
        "For every answer given correctly, marks of two points are given. A failed attempt attracts a deduction of one point.\n",
        "7. The fifth round also known as the riddle stage. There are four questions in this stage. There is one question for each subject(Math, chemistry, Physics\n",
        "and Biology). The riddle is given as a set of clues which. The clues are given to the contestants one after the other. If the answer to the riddle \n",
        "is given on the first clue, 5 points are awarded to the contestant. If the answer is given on the second clue, 4 points are awarded. 3 points are awarded\n",
        "on the subsequent clues. \n",
        "\n",
        "\n",
        "\n",
        "For questions that don't require calculations, the time required is 10 seconds and for questions that require calculations the time is 30seconds.\n",
        "These are the procedures you will use to coordinate the quiz competition.\n",
        "1. Your sole purpose is to train a single team.\n",
        "3. The team you are training is team1.\n",
        "2. Simulate the scores of other teams.\n",
        "3. Ask the questions by following the processes outlined previously\n",
        "4. Take the time into consideration when awarding marks. DO NOT give marks for late answers.\n",
        "5. Do not give points for wrong answers.\n",
        "6. If a team is not able to answer a question, pass the question to the next team. Stop passing the question if\n",
        "the question has been answered correctly. Stop passing the question if the question has been answered wrongly by all teams.\n",
        "7. Simulate the question and their responses for the other teams.\n",
        "8. When a team answers a question correctly, award the points to the team and move to the next question. Direct the question to\n",
        "the next team.\n",
        "9. Give remarks to the previous teams response. The reponse should be \"Correct for number of {number of points for answering that question}\"\n",
        "and for wrong answers, give the correct answer as a remark in the format \"Wrong. The correct answer is {correct answer}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Always return a json in this format:\n",
        "\"\n",
        "{\n",
        "\"current_round\":\"current round number\",\n",
        "\"current_question\":\"current question directed to a particular team\",\n",
        "\"question:\":\"question\",\n",
        "\"time\":\"time for the question in seconds\",\n",
        "\"current_question\":\"current question directed to a particular team\",\n",
        "\"quiz_mistress_remarks\":\"remarks\",\n",
        "\"question_directed_at_team\":\"team\",\n",
        "\"accumulated_points_for_team1\":\"points\",\n",
        "\"accumulated_points_for_team2\":\"points\",\n",
        "\"accumulated_points_for_team3\":\"points\",\n",
        "}\n",
        "\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "@app.post(\"/process\", response_model=OutputModel)\n",
        "async def process_data(input_data: InputModel):\n",
        "    ai_response = input_data.messages[0]\n",
        "    \n",
        "    return OutputModel(ai_response=ai_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Bc0gtOeFRK",
        "outputId": "0cef57a3-2980-41e7-f0fd-00603c14156e"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken # TO DO: Replace this comment with your ngronk token (can be obtained from your ngronk account)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgncu44zisvo",
        "outputId": "17f58b0f-01d3-48ba-bb79-6c81c9e53a55"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
