{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBsImOKYbMFe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C-YM1rzbF6a",
        "outputId": "8639aef1-edd6-40bc-dd2c-eaf5148e3b1d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BG0fM3XCa451",
        "outputId": "1cce67fe-2b91-4ce8-f149-bdcb29539e65"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install TTS\n",
        "!sudo apt-get install espeak-ng\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "\n",
        "# STT\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "\n",
        "# API-related dependencies\n",
        "!pip install fastapi uvicorn pydantic pyngrok nest_asyncio\n",
        "!pip install python-multipart\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoOhnH_hbauY"
      },
      "outputs": [],
      "source": [
        "# TTS-Related Imports\n",
        "import IPython\n",
        "import tempfile\n",
        "import subprocess\n",
        "import time\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.audio.numpy_transforms import save_wav\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# STT-Related Imports\n",
        "import io\n",
        "import wave\n",
        "import numpy as np\n",
        "import whisper\n",
        "import jiwer\n",
        "import time\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import joblib\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# API-Related Imports\n",
        "from fastapi import FastAPI,Response\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.responses import StreamingResponse,FileResponse\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import shutil\n",
        "from pydantic import BaseModel\n",
        "from IPython.display import Audio\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfOuc8Pxg8Y_"
      },
      "outputs": [],
      "source": [
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "app.add_middleware(GZipMiddleware, minimum_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlS247Nggdnb"
      },
      "source": [
        "## TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYUzxcTsbg17"
      },
      "outputs": [],
      "source": [
        "# Load TTS model\n",
        "live_config=VitsConfig()\n",
        "live_config.load_json(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/traineroutput/vits_vctk-May-24-2023_11+05PM-23a7a9a3/config.json\")\n",
        "live_vits = Vits.init_from_config(live_config)\n",
        "live_vits.load_onnx(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/elsie.onnx\")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NuZNL3ShiVG"
      },
      "outputs": [],
      "source": [
        "def live_audio(text:str):\n",
        "  text_inputs = np.asarray(\n",
        "      live_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "      dtype=np.int64,\n",
        "  )[None, :]\n",
        "  audio = live_vits.inference_onnx(text_inputs,speaker_id=0)\n",
        "  with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "    out_path = temp_file.name\n",
        "  save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "  return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWQ0Ci43goyB"
      },
      "outputs": [],
      "source": [
        "class LiveText(BaseModel):\n",
        "  text: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhC41XNvhZxr"
      },
      "outputs": [],
      "source": [
        "@app.get('/synthesize-speech')\n",
        "def onnx_audio(payload:LiveText):\n",
        "  out_path=live_audio(payload.text)\n",
        "  return FileResponse(out_path, media_type=\"audio/wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYnYzhfaghmB"
      },
      "source": [
        "## STT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YiZLETpqBP_",
        "outputId": "3b89c522-0271-4189-fa3f-da1ec549f456"
      },
      "outputs": [],
      "source": [
        "# Load STT Model\n",
        "# Load whisper model\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\"\n",
        "\n",
        "model = whisper.load_model(\"medium.en\", device = DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBgmmhkXqHOl"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio(path_to_audio):\n",
        "  \"\"\"Loads whisper model to transcribe audio\"\"\"\n",
        "\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(path_to_audio)\n",
        "\n",
        "  # Transcribe audio\n",
        "  result = model.transcribe(audio)\n",
        "\n",
        "  # Print transcript\n",
        "  return result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMFjRgyQrjrB"
      },
      "outputs": [],
      "source": [
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "@app.get(\"/get-transcript\")\n",
        "async def get_transcript(audio: AudioBytes):\n",
        "  try:\n",
        "    decoded_data = base64.b64decode(audio.data)\n",
        "\n",
        "    # Write bytes data to a .wav file\n",
        "    with io.BytesIO(decoded_data) as audio_file:\n",
        "        with wave.open(audio_file, \"wb\") as wav:\n",
        "          wav.setnchannels(1)\n",
        "          wav.setsampwidth(2)\n",
        "          wav.setframerate(16000)\n",
        "\n",
        "          # Write .wav files\n",
        "          wav.writeframes(decoded_data)\n",
        "\n",
        "    # Save the audio file with the custom name\n",
        "    audio_filename = audio.filename\n",
        "    with open(audio_filename, \"wb\") as file:\n",
        "        file.write(decoded_data)\n",
        "\n",
        "    transcript = transcribe_audio(audio_filename)\n",
        "    os.remove(audio_filename)\n",
        "    return {\"transcript\": transcript}\n",
        "  except Exception as e:\n",
        "    return {\"error\":str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LM fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 huggigface-hub==0.0.17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authenticate with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"model_id\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset and configure pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup for pushing our custom model to Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model.push_to_hub(\"Yourname/Llama-2-7b-chat-finetune\", check_pr=True)\n",
        "\n",
        "tokenizer.push_to_hub(\"Yourname/Llama-2-7b-chat-finetune\",check_pr=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure to use the required prompt template for the model you have trained. This will ensure that you get\n",
        "a reasonable text or chat completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example the required prompt template for llama-2 is:\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "{{ system_prompt }}\n",
        "<</SYS>>\n",
        "\n",
        "{{ user_message }} [/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_promt = \"\"\"\n",
        "You are a quiz mistress for the NSMQ quiz competition: \n",
        "\n",
        "This is how the competition occurs,\n",
        "1. There are three teams in the competition\n",
        "2. There are five rounds.\n",
        "3 The first round consists of 24 questions with each school getting two sets of questions from each subject: Math, Chemistry, Physics and Biology.\n",
        "Questions are asked for the different subjects one after the other with one question posed to each school from a set of questions from the same subject. \n",
        "For questions that don't require calculations, the time required is 10 seconds and for questions that require calculations the time is 30seconds.\n",
        "4. The second round consists of 8 questions with each school at liberty of answering immediately they have an answer.\n",
        "There are two questions each for each subject(Math, Chemistry, Physics and Biology) which any team can answer immediately the question is posed.\n",
        "For every wrong answer given, there is a deduction of one point from the accummulated scores.  Th efirst team to answer a question gets the full 3 points.\n",
        "The second team to make a correct attempt after the first team makes an attempt and gets it wrong, gets 2 points and if they make an unsuccessful attempt,\n",
        "the last team after making a successful attempt gets one point.  \n",
        "5. The third round also known as the problem of the day is such that a single question is posed to all three schools and each school has a maximum time of\n",
        "4 minutes to present a solution to the problem. Marks are awarded based on the criteria and presentation of the solution apart from the final answer itself.\n",
        "The total points to be accummulated is 10.\n",
        "6. The fourth round round is also known as the True or False stage. Here, two questions each from each subject(Math, chemistry, Physicics and Biology)\n",
        "are posed to each team. The questions are statements where each team is supposed to state whether the questions are True or False. \n",
        "For every answer given correctly, marks of two points are given. A failed attempt attracts a deduction of one point.\n",
        "7. The fifth round also known as the riddle stage. There are four questions in this stage. There is one question for each subject(Math, chemistry, Physics\n",
        "and Biology). The riddle is given as a set of clues which. The clues are given to the contestants one after the other. If the answer to the riddle \n",
        "is given on the first clue, 5 points are awarded to the contestant. If the answer is given on the second clue, 4 points are awarded. 3 points are awarded\n",
        "on the subsequent clues. \n",
        "\n",
        "\n",
        "\n",
        "For questions that don't require calculations, the time required is 10 seconds and for questions that require calculations the time is 30seconds.\n",
        "These are the procedures you will use to coordinate the quiz competition.\n",
        "1. Your sole purpose is to train a single team.\n",
        "3. The team you are training is team1.\n",
        "2. Simulate the scores of other teams.\n",
        "3. Ask the questions by following the processes outlined previously\n",
        "4. Take the time into consideration when awarding marks. DO NOT give marks for late answers.\n",
        "5. Do not give points for wrong answers.\n",
        "6. If a team is not able to answer a question, pass the question to the next team. Stop passing the question if\n",
        "the question has been answered correctly. Stop passing the question if the question has been answered wrongly by all teams.\n",
        "7. Simulate the question and their responses for the other teams.\n",
        "8. When a team answers a question correctly, award the points to the team and move to the next question. Direct the question to\n",
        "the next team.\n",
        "9. Give remarks to the previous teams response. The reponse should be \"That is Correct\"\n",
        "and for wrong answers, give the correct answer as a remark in the format \"Wrong. The correct answer is {correct answer}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Always return a json in this format:\n",
        "\"\n",
        "{\n",
        "\"current_round\":\"current round number\",\n",
        "\"current_question\":\"current question directed to a particular team\",\n",
        "\"question:\":\"question\",\n",
        "\"time\":\"time for the question in seconds\",\n",
        "\"current_question\":\"current question directed to a particular team\",\n",
        "\"quiz_mistress_remarks\":\"remarks\",\n",
        "\"question_directed_at_team\":\"team\",\n",
        "\"accumulated_points_for_team1\":\"points\",\n",
        "\"accumulated_points_for_team2\":\"points\",\n",
        "\"accumulated_points_for_team3\":\"points\",\n",
        "}\n",
        "\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "@app.post(\"/process\", response_model=OutputModel)\n",
        "async def process_data(input_data: InputModel):\n",
        "    ai_response = input_data.messages[0]\n",
        "    \n",
        "    return OutputModel(ai_response=ai_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Bc0gtOeFRK",
        "outputId": "0cef57a3-2980-41e7-f0fd-00603c14156e"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken # TO DO: Replace this comment with your ngronk token (can be obtained from your ngronk account)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgncu44zisvo",
        "outputId": "17f58b0f-01d3-48ba-bb79-6c81c9e53a55"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
